{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import joblib\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('diabetes.csv')  # Replace with your actual file path\n",
        "\n",
        "# Step 1: Create BMI_category column\n",
        "def categorize_bmi(bmi):\n",
        "    if bmi < 18.5:\n",
        "        return 'Underweight'\n",
        "    elif 18.5 <= bmi < 24.9:\n",
        "        return 'Normal weight'\n",
        "    elif 25 <= bmi < 29.9:\n",
        "        return 'Overweight'\n",
        "    else:\n",
        "        return 'Obesity'\n",
        "\n",
        "data['BMI_category'] = data['BMI'].apply(categorize_bmi)\n",
        "\n",
        "# Step 2: Split the data into train and validation sets\n",
        "X = data.drop(columns=['BMI_category', 'Outcome'])  # Features\n",
        "y = data['Outcome']  # Target\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Apply Standard Scaler on numeric features\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Step 4: Apply One-Hot Encoding on categorical features\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "\n",
        "\n",
        "# Preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', scaler, numeric_features),\n",
        "        ('cat', encoder, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Step 5: KNN Classifier with different k values\n",
        "knn_scores = {}\n",
        "for k in [3, 5, 7]:\n",
        "    knn = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', KNeighborsClassifier(n_neighbors=k))\n",
        "    ])\n",
        "\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_val)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    knn_scores[k] = f1\n",
        "\n",
        "best_k = max(knn_scores, key=knn_scores.get)\n",
        "print(f'Best KNN k: {best_k}, F1 Score: {knn_scores[best_k]}')\n",
        "\n",
        "# Step 6: Decision Tree Classifier with different max_depth values\n",
        "dt_scores = {}\n",
        "for max_depth in [3, 5, 7]:\n",
        "    dt = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', DecisionTreeClassifier(max_depth=max_depth))\n",
        "    ])\n",
        "\n",
        "    dt.fit(X_train, y_train)\n",
        "    y_pred = dt.predict(X_val)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    dt_scores[max_depth] = f1\n",
        "\n",
        "best_depth = max(dt_scores, key=dt_scores.get)\n",
        "print(f'Best Decision Tree max_depth: {best_depth}, F1 Score: {dt_scores[best_depth]}')\n",
        "\n",
        "# Step 7: Build the inference pipeline\n",
        "best_model = KNeighborsClassifier(n_neighbors=best_k)  # Or DecisionTreeClassifier(max_depth=best_depth)\n",
        "inference_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', best_model)\n",
        "])\n",
        "\n",
        "# Fit the best model on the training data\n",
        "inference_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Save the scaler, encoder, and model\n",
        "joblib.dump(inference_pipeline.named_steps['preprocessor'], 'preprocessor.pkl')\n",
        "joblib.dump(inference_pipeline.named_steps['classifier'], 'best_model.pkl')\n",
        "\n",
        "# Inference script example:\n",
        "# Load the saved components\n",
        "scaler = joblib.load('preprocessor.pkl')\n",
        "model = joblib.load('best_model.pkl')\n",
        "\n",
        "# Apply the transformations and make predictions on new test data\n",
        "sample = X_val.iloc[0:5]  # Replace with actual test samples\n",
        "sample_transformed = scaler.transform(sample)\n",
        "predictions = model.predict(sample_transformed)\n",
        "\n",
        "print(f'Predictions: {predictions}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXdcoS6Q268u",
        "outputId": "52da090d-c774-444b-d1de-de99d8472935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best KNN k: 3, F1 Score: 0.5544554455445545\n",
            "Best Decision Tree max_depth: 5, F1 Score: 0.6862745098039216\n",
            "Predictions: [0 0 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To improve the model, we'll apply a few techniques that can help:**\n",
        "\n",
        "**Handle Class Imbalance:** We'll use SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset by generating synthetic samples for the minority class.\n",
        "\n",
        "**Hyperparameter Tuning:** We'll use GridSearchCV to tune the hyperparameters of the Decision Tree and KNN models.\n",
        "\n",
        "**Cross-Validation:** Use cross-validation to evaluate the models more reliably.\n",
        "\n",
        "**SMOTE:** Applied SMOTE to balance the dataset before training. This generates synthetic data for the minority class to reduce the class imbalance problem.\n",
        "\n",
        "**GridSearchCV:** Used GridSearchCV for hyperparameter tuning to find the best parameters for both the KNN and Decision Tree classifiers. The cv=5 performs 5-fold cross-validation.\n",
        "\n",
        "**Cross-Validation:** Evaluated both models with cross-validation to get a more reliable estimate of their performance.\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "After running this, check the F1 scores for both models after hyperparameter tuning.\n",
        "\n",
        "The one with the higher F1 score will be chosen as the best model.\n",
        "\n",
        "The best model will be saved, and predictions will be made on the test samples."
      ],
      "metadata": {
        "id": "vryegBmW3Jf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import joblib\n",
        "\n",
        "# Load the diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "data = pd.read_csv(url, names=columns)\n",
        "\n",
        "# 1. Create BMI category\n",
        "def bmi_category(bmi):\n",
        "    if bmi < 18.5:\n",
        "        return 'Underweight'\n",
        "    elif 18.5 <= bmi < 24.9:\n",
        "        return 'Normal'\n",
        "    elif 25 <= bmi < 29.9:\n",
        "        return 'Overweight'\n",
        "    else:\n",
        "        return 'Obese'\n",
        "\n",
        "data['BMI_category'] = data['BMI'].apply(bmi_category)\n",
        "\n",
        "# 2. Split data into train and val sets\n",
        "X = data.drop('Outcome', axis=1)\n",
        "y = data['Outcome']\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Apply Standard Scaler on numeric features\n",
        "numeric_features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# 4. Apply One-Hot Encoding on categorical features\n",
        "categorical_features = ['BMI_category']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# 5. Combine transformers into a ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# 6. Apply SMOTE after preprocessing\n",
        "# Apply preprocessing to both training and validation sets\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "X_val_transformed = preprocessor.transform(X_val)\n",
        "\n",
        "# Apply SMOTE to the transformed training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_transformed, y_train)\n",
        "\n",
        "# 7. Hyperparameter Tuning using GridSearchCV for KNN and Decision Tree\n",
        "# KNN GridSearch\n",
        "knn_model = Pipeline(steps=[\n",
        "    ('classifier', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "knn_param_grid = {'classifier__n_neighbors': [3, 5, 7, 9]}\n",
        "knn_grid_search = GridSearchCV(knn_model, knn_param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "\n",
        "# Fit the grid search on the resampled data\n",
        "knn_grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Decision Tree GridSearch\n",
        "dt_model = Pipeline(steps=[\n",
        "    ('classifier', DecisionTreeClassifier())\n",
        "])\n",
        "\n",
        "dt_param_grid = {'classifier__max_depth': [3, 5, 7, 9], 'classifier__min_samples_split': [2, 5, 10]}\n",
        "dt_grid_search = GridSearchCV(dt_model, dt_param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "\n",
        "# Fit the grid search on the resampled data\n",
        "dt_grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# 8. Evaluate both models using cross-validation and F1 score\n",
        "knn_best_model = knn_grid_search.best_estimator_\n",
        "dt_best_model = dt_grid_search.best_estimator_\n",
        "\n",
        "knn_f1 = cross_val_score(knn_best_model, X_train_resampled, y_train_resampled, cv=5, scoring='f1').mean()\n",
        "dt_f1 = cross_val_score(dt_best_model, X_train_resampled, y_train_resampled, cv=5, scoring='f1').mean()\n",
        "\n",
        "print(f\"Best KNN F1 score (after tuning): {knn_f1}\")\n",
        "print(f\"Best Decision Tree F1 score (after tuning): {dt_f1}\")\n",
        "\n",
        "# 9. Save the best model and preprocessing pipeline\n",
        "best_model = knn_best_model if knn_f1 > dt_f1 else dt_best_model\n",
        "best_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Save the preprocessor, encoder, and model\n",
        "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
        "joblib.dump(best_model.named_steps['classifier'], 'best_model.pkl')\n",
        "\n",
        "# 10. Load and apply the saved models on test sample\n",
        "preprocessor = joblib.load('preprocessor.pkl')\n",
        "model = joblib.load('best_model.pkl')\n",
        "\n",
        "# Example: apply on 5 validation samples\n",
        "test_samples = X_val.head(5)\n",
        "test_samples_transformed = preprocessor.transform(test_samples)\n",
        "predictions = model.predict(test_samples_transformed)\n",
        "\n",
        "print(\"Predictions for 5 test samples:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAJYlH4R2XMX",
        "outputId": "49b3e344-be6d-4515-a22c-380e239d207f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best KNN F1 score (after tuning): 0.8233687898240785\n",
            "Best Decision Tree F1 score (after tuning): 0.7767446727757633\n",
            "Predictions for 5 test samples: [0 0 1 0 1]\n"
          ]
        }
      ]
    }
  ]
}